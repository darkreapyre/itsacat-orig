{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serverless Neural Network Introduction\n",
    "## Architecture Overview\n",
    "\n",
    "The **“Serverless Neural Network”** is a solution for generating a Cloud Native image classifier, using idempotent Amazon Web Services ([AWS](https://aws.amazon.com/what-is-aws/)) Lambda Functions. The **\"SNN\"** is used to train a model to predict whether a particular image is a **“cat”** vs. **“non-cat”** and fits within an overall prediction pipeline as the model training process. The primary objective of using Lambda Functions as opposed to other services like **SageMaker** or dedicated Machine Leaning Frameworks like **MXNet **or ** TensorFlow**, is to remove the abstraction layer that inevitably perpetuates the concept that Neural Networks or Deep Learning is a “black box” architecture and thus somewhat difficult to understand.\n",
    "\n",
    "<img src=\"images/Prediction_Architecture.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><center>**Machine Learning Pipeline**</center></caption><br>\n",
    "\n",
    "By simulating the individual Neurons and the mathematical functions they perform, it's easier to learn the exactly what each neuron is doing and how they contribute to optimizing (or “learning”) the overall hyper-parameters used for the final prediction model. Additionally, leveraging this framework for model training will hopefully provide a more in-depth understanding of how each neuron deals with the \"vectorized\" matrix calculations during Forward Propagation process **AND** the gradient derivative calculations during the Backward Propagation process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Overview\n",
    "\n",
    "<img src=\"images/2layerNN_kiank.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Neural Network Model (shown above) can be summarized as:\n",
    "  \n",
    "**INPUT --> LINEAR/RELU --> LINEAR/SIGMOID --> OUTPUT**  \n",
    "\n",
    "- The **Input** is a $(64, 64, 3)$ image what is flattened to a vector $(12288, 1)$. See the **Data Overview** Section.\n",
    "- The corresponding vector: $[x_{0}, x_{1}, \\dots, x_{12287}]^T$ is then multiplied by the **weight matrix** $W^{[1]}$ of size $(n^{[1]}, 12288)$.\n",
    "- The **bias** term is then added to take the **Relu** (non-linear activation) to get a vector of size $[a^{[1]}_0, a^{[2]}_1, \\dots, a^{[1]}_{n^{[1]}-1}]^T$.\n",
    "- The process is then repeated for the next layer, by taking the resulting vector and multiplying it by the weight matrix $W^{[2]}$ and then adding the intercept (**bias**).\n",
    "- Lastly, the **sigmoid** activation is applied to the result. If the result is greater then $0.5$, it is classified as a **Cat**.\n",
    "\n",
    "Therefore, the **Network Model Parameters** (*parameters.json*) for the above process are as follows:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"epochs\": 10,\n",
    "    \"layers\": 2,\n",
    "    \"activations\": {\n",
    "        \"layer1\": \"relu\",\n",
    "        \"layer2\": \"sigmoid\"\n",
    "    },\n",
    "    \"neurons\": {\n",
    "        \"layer1\": 3,\n",
    "        \"layer2\": 1\n",
    "    },\n",
    "    \"learning_rate\": 0.0075\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Implementation\n",
    "To implement the Neural Network using the **SNN** framework and the above Network configuration, the workflow is comprised of five key steps:  \n",
    "1. Network Initialization.\n",
    "2. Forward Propagation.\n",
    "3. Calculate the Loss (Cost Function).\n",
    "4. Backward Propagation.\n",
    "5. Parameter Optimization (Gradient Descent).\n",
    "\n",
    "The outcome of the above stages provides the optimal model parameters, for use in final prediction, as can be seen in the process diagram below.  \n",
    "\n",
    "<img src=\"images/final_outline.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>\n",
    "\n",
    "The next sections will further describe each phase in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization\n",
    "For an **L-Layer** network, the *Weights* and *Bias* must be initialized for each individual layer, therefore the dimensions for these matrices must match to the dimensions of each layer. For example, if $n^{[l]}$ is the number of hidden units (neurons) in layer $l$ and the size of the input $X$ is $(12288, 209)$, for $m = 209$ training examples, then:\n",
    "\n",
    "\n",
    "|               \t|      **Shape of W**      \t|  **Shape of b**  \t|                 **Activation**                \t| **Shape of Activation** \t|\n",
    "|---------------\t|:------------------------:\t|:----------------:\t|:---------------------------------------------:\t|:-----------------------:\t|\n",
    "| **Layer 1**   \t| $(n^{[1]},12288)$        \t| $(n^{[1]},1)$    \t| $Z^{[1]} = W^{[1]},X + b^{[1]}$               \t| $(n^{[1]},209)$         \t|\n",
    "| **Layer 2**   \t| $(n^{[2]}, n^{[1]})$     \t| $(n^{[2]},1)$    \t| $Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$         \t| $(n^{[2]}, 209)$        \t|\n",
    "| $\\vdots$      \t| $\\vdots$                 \t| $\\vdots$         \t| $\\vdots$                                      \t| $\\vdots$                \t|\n",
    "| **Layer L-1** \t| $(n^{[L-1]}, n^{[L-2]})$ \t| $(n^{[L-1]}, 1)$ \t| $Z^{[L-1]} =,W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ \t| $(n^{[L-1]}, 209)$      \t|\n",
    "| **Layer L**   \t| $(n^{[L]}, n^{[L-1]})$   \t| $(n^{[L]}, 1)$   \t| $Z^{[L]} =,W^{[L]} A^{[L-1]} + b^{[L]}$       \t| $(n^{[L]}, 209)$        \t|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute $W X + b$ in python, it carries out broadcasting. For example, if:  \n",
    "\n",
    "$$\n",
    "W = \\begin{bmatrix}\n",
    "j  & k  & l \\\\\\\n",
    "m  & n & o  \\\\\\\n",
    "p  & q & r\n",
    "\\end{bmatrix}\n",
    "\\space\n",
    "\\space\n",
    "X = \\begin{bmatrix}\n",
    "a  & b  & c \\\\\\\n",
    "d  & e & f \\\\\\\n",
    "g  & h & i \n",
    "\\end{bmatrix}\n",
    "\\space\n",
    "\\space\n",
    "b =\\begin{bmatrix}\n",
    "s  \\\\\\\n",
    "t  \\\\\\\n",
    "u\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "Then $WX + b$ will be:\n",
    "\n",
    "$$\n",
    "WX + b = \\begin{bmatrix}\n",
    "(ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\\\n",
    "(ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\\\n",
    "(pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start the initialization process, the *Weights* are initialized randomly using a \"standard\" normal distribution with a mean of $0$ and a standard deviation of $1$. To further constrain the weights to be close to zero **but** not exactly zero (for *symmetry breaking*), each random weight is multiplied by $0.01$. The *Bias* is initialized to zero but also multiplied by $0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "The *Forward Propagation* step of the process is comprised of two separate pieces, the **Linear** activation and the **Non-Linear** activation to constrain the outputs between $0$ and $1$.  \n",
    "\n",
    "#### Linear Activation\n",
    "The *Linear* part of the activation computes the following equation:  \n",
    "$$Z^{[l]} = W^{[l]} \\cdot A^{[l]} + b^{[l]}$$\n",
    "\n",
    ">**Note:** It is important to cache the Linear Activations ($Z$) for later use in he Backward Propagation process.\n",
    "\n",
    "Where $A^{[0]} = X$\n",
    "\n",
    "#### Non-Linear Activation\n",
    "The **L-Layer** Neural Network implements two differnt non-linear activation functions:  \n",
    "\n",
    "- **Rectified Linear Unit (ReLU):** The mathematical formula for the *ReLU* function is $A = ReLU(Z) = max(0, Z)$.  \n",
    "- **Sigmoid:** The methematical formula for the *Sigmoid* function is $\\sigma(Z) = \\sigma(W\\cdot A+b) = \\frac{1}{1 + e^{(-z)}}$.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss\n",
    "**Cross Entropy** is commonly-used in binary classification (labels are assumed to take values $0$ or $1$) as a loss function which is computed by:\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\big[y^{(i)}\\cdot\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\cdot\\log\\left(1-a^{[L](i)}\\right)\\big]$$\n",
    "\n",
    "Where $a^{[L](i)}$ is the last layer of the network and is synonymous with $\\hat{y}$.\n",
    "\n",
    "Cross entropy measures the divergence between two probability distribution, if the cross entropy is large, which means that the difference between two distribution is large, while if the cross entropy is small, which means that two distribution is similar to each other. Generally, comparing to quadratic cost function, cross entropy cost function has the advantages that fast convergence and is more likely to reach the global optimization. For the mathematical details, see [wikipedia](https://en.wikipedia.org/wiki/Cross_entropy)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation\n",
    "*Backward Propagation* is used to calculate the the gradient of the *Loss* function with respect to the various paramaters, as follows:\n",
    "\n",
    "<img src=\"images/backprop_kiank.png\" style=\"width:800px;height:500px;\">\n",
    "<caption><left>[*image source](https://www.deeplearning.ai)</left></caption><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Non-Linear Derivative\n",
    "As with *Forward Propagation*, there are two derivative non-linear activation functions for *Sigmoid* and *ReLU* respectively. If $g(\\cdot)$ is the activations function, then the derivative of *Sigmoid* and *ReLU* compute:\n",
    "$$dZ^{[l]} = \\frac{\\partial\\mathcal{L}}{\\partial Z^{[l]}} = dA^{[l]} \\cdot g^{'}(Z^{[l]})$$\n",
    "\n",
    "#### Linear Derivative\n",
    "Once the derivative of the non-linear activation is computed, the derivatives of $W^{[l]}$, $b^{[l]}$ and $A^{[l]}$, are computed using the input $dZ^{[l]}$, to get , $dW^{[l]}$, $db^{[l]}$, $dA^{[l-1]}$ as follows:\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$  \n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$  \n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Update using Gradient Descent\n",
    "The Model parameters ($W^{[l]}$ and $b^{[l]}$) are updated using **Gradient Descent** using the following formula:  \n",
    "\n",
    "$$W^{[l]} = W^{[l]} - \\alpha\\cdot dW^{[l]}$$  \n",
    "$$b^{[l]} = b^{[l]} - \\alpha\\cdot db^{[l]}$$  \n",
    "\n",
    "Where $\\alpha$ is the *Learning Rate*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction\n",
    "After the fitted parameters are updated using *Gradient Descent*, the paramaters can be used to predict wether a new image can classified as a **cat** or **non-cat** image. For further information on how the accuracy of the trained model fairs against testing data or unseen data, see the **Analysis** Notebook.\n",
    "\n",
    "---\n",
    "## Next: Code Overview\n",
    "Now that the **Serverless Neural Network** has beenn introduced, it's time to review the Python code that makes up the various Lambda Functions in the [**Codebook**](./Codebook.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
